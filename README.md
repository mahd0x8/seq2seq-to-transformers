# seq2seq-to-transformers
A hands-on learning repo exploring the foundations of LLMs â€” from sequence-to-sequence models using RNNs, LSTMs, and GRUs to attention and transformer-based architectures. Includes code, experiments, and step-by-step explanations.
